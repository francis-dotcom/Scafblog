# Building GlobalConnect: A Complete Guide to Multi-Region Financial Microservices on AWS

<div className="hero-callout">
  <span className="bulb">üèóÔ∏è</span>
  <strong>
    The step-by-step blueprint for building enterprise-grade, multi-region
    financial services infrastructure that scales to millions of transactions.
  </strong>
</div>

## Prerequisites & What You'll Build üéØ

By the end of this tutorial, you'll have deployed a production-ready financial services platform spanning **3 AWS regions** with **40+ microservices**, complete disaster recovery, and enterprise-grade security.

<div className="prerequisites-grid">
  <div className="prereq-card technical">
    <h4>üõ†Ô∏è Technical Requirements</h4>
    <ul>
      <li>AWS Account with admin access</li>
      <li>Terraform v1.0+ installed</li>
      <li>kubectl and eksctl CLI tools</li>
      <li>Docker Desktop running</li>
      <li>Basic Kubernetes knowledge</li>
    </ul>
  </div>

{" "}

<div className="prereq-card time">
  <h4>‚è±Ô∏è Time Investment</h4>
  <ul>
    <li>
      <strong>Total:</strong> 8-12 hours
    </li>
    <li>
      <strong>Phase 1-2:</strong> 3-4 hours (Infrastructure)
    </li>
    <li>
      <strong>Phase 3-4:</strong> 3-4 hours (Services)
    </li>
    <li>
      <strong>Phase 5-6:</strong> 2-4 hours (Monitoring & DR)
    </li>
  </ul>
</div>

  <div className="prereq-card cost">
    <h4>üí∞ AWS Costs</h4>
    <ul>
      <li>
        <strong>Development:</strong> $200-400/month
      </li>
      <li>
        <strong>Production-scale:</strong> $2,000-5,000/month
      </li>
      <li>
        <strong>Free tier eligible:</strong> Some services
      </li>
      <li>
        <strong>Cost optimization:</strong> Included in tutorial
      </li>
    </ul>
  </div>
</div>

:::tip TUTORIAL APPROACH
This tutorial follows **infrastructure-as-code** principles with **production-ready configurations**. Every component includes security hardening, monitoring, and disaster recovery considerations.
:::

## Architecture Overview üèõÔ∏è

<div className="arch-stats">
  <div className="arch-stat"><strong>3</strong> AWS Regions</div>
  <div className="arch-stat"><strong>40+</strong> Microservices</div>
  <div className="arch-stat"><strong>15min</strong> RTO Target</div>
  <div className="arch-stat"><strong>99.99%</strong> Availability</div>
</div>
.arch-stat {
  font-size: 1.1rem;
  margin: 0.5rem 0;
}
.arch-stat strong {
  color: #ff9900;
}

### üó∫Ô∏è **Regional Strategy**

<div className="region-strategy">
  <div className="strategy-region primary">
    <h4>üá∫üá∏ US-East-1 (Primary)</h4>
    <p>
      <strong>Role:</strong> Main trading operations and customer-facing
      services
    </p>
    <p>
      <strong>Services:</strong> All 40+ microservices running at full capacity
    </p>
    <p>
      <strong>Data:</strong> Primary databases with real-time replication
    </p>
  </div>

{" "}

<div className="strategy-region secondary">
  <h4>üåä US-West-2 (Secondary)</h4>
  <p>
    <strong>Role:</strong> Hot standby + West Coast low-latency operations
  </p>
  <p>
    <strong>Services:</strong> Critical services running, others in standby
  </p>
  <p>
    <strong>Data:</strong> Read replicas with &lt;5min RPO
  </p>
</div>

  <div className="strategy-region tertiary">
    <h4>üá™üá∫ EU-West-1 (Compliance)</h4>
    <p>
      <strong>Role:</strong> European operations with GDPR compliance
    </p>
    <p>
      <strong>Services:</strong> EU-specific services + data residency
    </p>
    <p>
      <strong>Data:</strong> Isolated EU data with local encryption keys
    </p>
  </div>
</div>

---

## Phase 1: Foundation - AWS Multi-Region Setup üèóÔ∏è

### Step 1.1: AWS CLI & Profile Configuration

First, set up your AWS environment with separate profiles for each region:

```bash
# Install AWS CLI (if not already installed)
# macOS
brew install awscli

# Linux
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip && sudo ./aws/install

# Configure AWS profiles for each region
aws configure --profile globalconnect-primary
# AWS Access Key ID: [your-key]
# AWS Secret Access Key: [your-secret]
# Default region name: us-east-1
# Default output format: json

aws configure --profile globalconnect-secondary
# Region: us-west-2

aws configure --profile globalconnect-eu
# Region: eu-west-1
```

### Step 1.2: Terraform Infrastructure Setup

Create the foundational infrastructure using Terraform:

```bash
# Install Terraform
brew install terraform  # macOS
# OR
wget https://releases.hashicorp.com/terraform/1.6.0/terraform_1.6.0_linux_amd64.zip

# Create project structure
mkdir -p globalconnect-infrastructure/{modules,environments}
cd globalconnect-infrastructure
```

<div className="file-structure">

**üìÅ Project Structure:**

```
globalconnect-infrastructure/
‚îú‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ variables.tf
‚îú‚îÄ‚îÄ outputs.tf
‚îú‚îÄ‚îÄ terraform.tfvars
‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îú‚îÄ‚îÄ vpc/
‚îÇ   ‚îú‚îÄ‚îÄ eks/
‚îÇ   ‚îú‚îÄ‚îÄ rds/
‚îÇ   ‚îî‚îÄ‚îÄ security/
‚îî‚îÄ‚îÄ environments/
    ‚îú‚îÄ‚îÄ dev/
    ‚îú‚îÄ‚îÄ staging/
    ‚îî‚îÄ‚îÄ prod/
```

</div>

### Step 1.3: Core VPC Configuration

Create the multi-region VPC setup:

```hcl
# main.tf
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

# Provider configurations for each region
provider "aws" {
  alias  = "us_east_1"
  region = "us-east-1"
  profile = "globalconnect-primary"

  default_tags {
    tags = {
      Project     = "GlobalConnect"
      Environment = var.environment
      ManagedBy   = "Terraform"
    }
  }
}

provider "aws" {
  alias  = "us_west_2"
  region = "us-west-2"
  profile = "globalconnect-secondary"

  default_tags {
    tags = {
      Project     = "GlobalConnect"
      Environment = var.environment
      ManagedBy   = "Terraform"
    }
  }
}

provider "aws" {
  alias  = "eu_west_1"
  region = "eu-west-1"
  profile = "globalconnect-eu"

  default_tags {
    tags = {
      Project     = "GlobalConnect"
      Environment = var.environment
      ManagedBy   = "Terraform"
    }
  }
}

# Primary VPC (US-East-1)
module "primary_vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  providers = {
    aws = aws.us_east_1
  }

  name = "globalconnect-primary-vpc"
  cidr = "10.1.0.0/16"

  azs = ["us-east-1a", "us-east-1b", "us-east-1c"]

  # Subnets
  private_subnets  = ["10.1.1.0/24", "10.1.2.0/24", "10.1.3.0/24"]
  public_subnets   = ["10.1.101.0/24", "10.1.102.0/24", "10.1.103.0/24"]
  database_subnets = ["10.1.201.0/24", "10.1.202.0/24", "10.1.203.0/24"]

  # NAT Gateway configuration
  enable_nat_gateway = true
  enable_vpn_gateway = false
  single_nat_gateway = false  # Multi-AZ for HA

  # DNS
  enable_dns_hostnames = true
  enable_dns_support   = true

  # VPC Flow Logs for security monitoring
  enable_flow_log                      = true
  create_flow_log_cloudwatch_log_group = true
  create_flow_log_cloudwatch_iam_role  = true

  tags = {
    Region = "primary"
    Role   = "trading-operations"
  }
}

# Secondary VPC (US-West-2) - Similar configuration
module "secondary_vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  providers = {
    aws = aws.us_west_2
  }

  name = "globalconnect-secondary-vpc"
  cidr = "10.2.0.0/16"

  azs = ["us-west-2a", "us-west-2b", "us-west-2c"]

  private_subnets  = ["10.2.1.0/24", "10.2.2.0/24", "10.2.3.0/24"]
  public_subnets   = ["10.2.101.0/24", "10.2.102.0/24", "10.2.103.0/24"]
  database_subnets = ["10.2.201.0/24", "10.2.202.0/24", "10.2.203.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = false

  enable_dns_hostnames = true
  enable_dns_support   = true

  enable_flow_log                      = true
  create_flow_log_cloudwatch_log_group = true
  create_flow_log_cloudwatch_iam_role  = true

  tags = {
    Region = "secondary"
    Role   = "disaster-recovery"
  }
}

# EU VPC (EU-West-1) - GDPR Compliant
module "eu_vpc" {
  source = "terraform-aws-modules/vpc/aws"
  version = "~> 5.0"

  providers = {
    aws = aws.eu_west_1
  }

  name = "globalconnect-eu-vpc"
  cidr = "10.3.0.0/16"

  azs = ["eu-west-1a", "eu-west-1b", "eu-west-1c"]

  private_subnets  = ["10.3.1.0/24", "10.3.2.0/24", "10.3.3.0/24"]
  public_subnets   = ["10.3.101.0/24", "10.3.102.0/24", "10.3.103.0/24"]
  database_subnets = ["10.3.201.0/24", "10.3.202.0/24", "10.3.203.0/24"]

  enable_nat_gateway = true
  single_nat_gateway = false

  enable_dns_hostnames = true
  enable_dns_support   = true

  enable_flow_log                      = true
  create_flow_log_cloudwatch_log_group = true
  create_flow_log_cloudwatch_iam_role  = true

  tags = {
    Region = "eu"
    Role   = "gdpr-compliance"
    DataResidency = "eu-only"
  }
}
```

### Step 1.4: Deploy Foundation

```bash
# Initialize and validate Terraform
terraform init
terraform validate
terraform plan

# Deploy infrastructure
terraform apply
```

<div className="progress-indicator">
  <div className="progress-step completed">
    <span className="step-number">1</span>
    <span className="step-title">Multi-Region VPCs</span>
  </div>
  <div className="progress-step">
    <span className="step-number">2</span>
    <span className="step-title">EKS Clusters</span>
  </div>
  <div className="progress-step">
    <span className="step-number">3</span>
    <span className="step-title">Microservices</span>
  </div>
</div>

---

## Phase 2: Kubernetes Infrastructure - EKS Clusters üö¢

### Step 2.1: Install Kubernetes Tools

```bash
# Install eksctl (AWS EKS CLI)
# macOS
brew install eksctl

# Linux
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv /tmp/eksctl /usr/local/bin

# Install kubectl
brew install kubectl  # macOS
# OR for Linux:
curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

# Install Helm (Kubernetes package manager)
brew install helm  # macOS
```

### Step 2.2: EKS Cluster Configuration

Create production-ready EKS clusters with proper security and scaling:

```yaml
# eks-primary-cluster.yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: globalconnect-primary
  region: us-east-1
  version: "1.28"

# VPC Configuration (use existing VPC from Terraform)
vpc:
  id: "${VPC_ID}" # Will be replaced with actual VPC ID
  subnets:
    private:
      us-east-1a: { id: "${PRIVATE_SUBNET_1A}" }
      us-east-1b: { id: "${PRIVATE_SUBNET_1B}" }
      us-east-1c: { id: "${PRIVATE_SUBNET_1C}" }
    public:
      us-east-1a: { id: "${PUBLIC_SUBNET_1A}" }
      us-east-1b: { id: "${PUBLIC_SUBNET_1B}" }
      us-east-1c: { id: "${PUBLIC_SUBNET_1C}" }

# IAM settings
iam:
  withOIDC: true
  serviceAccounts:
    - metadata:
        name: aws-load-balancer-controller
        namespace: kube-system
      wellKnownPolicies:
        awsLoadBalancerController: true
    - metadata:
        name: cluster-autoscaler
        namespace: kube-system
      wellKnownPolicies:
        autoScaler: true

# Node Groups
managedNodeGroups:
  - name: trading-nodes
    instanceType: c5.2xlarge # High-performance for trading workloads
    desiredCapacity: 3
    minSize: 2
    maxSize: 10

    # Security
    ami: auto
    amiFamily: AmazonLinux2

    # Networking
    privateNetworking: true

    # Scaling
    iam:
      attachPolicyARNs:
        - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
        - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
        - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
        - arn:aws:iam::aws:policy/CloudWatchAgentServerPolicy

    labels:
      role: trading
      environment: production

    tags:
      nodegroup-role: trading-operations

  - name: general-nodes
    instanceType: m5.large
    desiredCapacity: 4
    minSize: 2
    maxSize: 20

    privateNetworking: true

    labels:
      role: general
      environment: production

    tags:
      nodegroup-role: general-workloads

# Logging
cloudWatch:
  clusterLogging:
    enableTypes: ["*"]

# Add-ons
addons:
  - name: vpc-cni
    version: latest
  - name: coredns
    version: latest
  - name: kube-proxy
    version: latest
  - name: aws-ebs-csi-driver
    version: latest
```

### Step 2.3: Deploy EKS Clusters

```bash
# Get VPC details from Terraform output
export VPC_ID=$(terraform output -raw primary_vpc_id)
export PRIVATE_SUBNET_1A=$(terraform output -raw primary_private_subnets | jq -r '.[0]')
# ... (get other subnet IDs)

# Substitute variables in cluster config
envsubst < eks-primary-cluster.yaml > eks-primary-cluster-filled.yaml

# Create primary cluster
eksctl create cluster -f eks-primary-cluster-filled.yaml

# Create secondary cluster (similar process)
eksctl create cluster --name=globalconnect-secondary \
  --region=us-west-2 \
  --version=1.28 \
  --nodes=3 \
  --node-type=m5.large \
  --managed

# Create EU cluster
eksctl create cluster --name=globalconnect-eu \
  --region=eu-west-1 \
  --version=1.28 \
  --nodes=3 \
  --node-type=m5.large \
  --managed
```

### Step 2.4: Configure kubectl Contexts

```bash
# Update kubeconfig for all clusters
aws eks --region us-east-1 update-kubeconfig --name globalconnect-primary --alias primary
aws eks --region us-west-2 update-kubeconfig --name globalconnect-secondary --alias secondary
aws eks --region eu-west-1 update-kubeconfig --name globalconnect-eu --alias eu

# Verify access
kubectl get nodes --context=primary
kubectl get nodes --context=secondary
kubectl get nodes --context=eu

# Set primary as default context
kubectl config use-context primary
```

<div className="progress-indicator">
  <div className="progress-step completed">
    <span className="step-number">1</span>
    <span className="step-title">Multi-Region VPCs</span>
  </div>
  <div className="progress-step completed">
    <span className="step-number">2</span>
    <span className="step-title">EKS Clusters</span>
  </div>
  <div className="progress-step">
    <span className="step-number">3</span>
    <span className="step-title">Microservices</span>
  </div>
</div>

---

## Phase 3: Core Services Deployment üîß

### Step 3.1: Install Service Mesh (Istio)

```bash
# Install Istio
curl -L https://istio.io/downloadIstio | sh -
export PATH="$PATH:$PWD/istio-1.19.3/bin"

# Install Istio on primary cluster
kubectl config use-context primary
istioctl install --set values.global.meshID=mesh1 --set values.global.network=primary-network

# Enable automatic sidecar injection
kubectl label namespace default istio-injection=enabled

# Install on secondary cluster
kubectl config use-context secondary
istioctl install --set values.global.meshID=mesh1 --set values.global.network=secondary-network

# Install on EU cluster
kubectl config use-context eu
istioctl install --set values.global.meshID=mesh1 --set values.global.network=eu-network
```

### Step 3.2: Deploy Event Streaming (Kafka)

```bash
# Add Helm repositories
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo add strimzi https://strimzi.io/charts/
helm repo update

# Create namespace
kubectl create namespace kafka --context=primary

# Deploy Kafka using Strimzi operator (production-ready)
helm install strimzi-kafka-operator strimzi/strimzi-kafka-operator \
  --namespace kafka \
  --context=primary
```

Create Kafka cluster configuration:

```yaml
# kafka-cluster.yaml
apiVersion: kafka.strimzi.io/v1beta2
kind: Kafka
metadata:
  name: globalconnect-kafka
  namespace: kafka
spec:
  kafka:
    version: 3.5.0
    replicas: 3

    listeners:
      - name: plain
        port: 9092
        type: internal
        tls: false
      - name: tls
        port: 9093
        type: internal
        tls: true

    config:
      offsets.topic.replication.factor: 3
      transaction.state.log.replication.factor: 3
      transaction.state.log.min.isr: 2
      default.replication.factor: 3
      min.insync.replicas: 2
      inter.broker.protocol.version: "3.5"

    storage:
      type: jbod
      volumes:
        - id: 0
          type: persistent-claim
          size: 100Gi
          deleteClaim: false
          class: gp3

    resources:
      requests:
        memory: 2Gi
        cpu: 500m
      limits:
        memory: 4Gi
        cpu: 1000m

  zookeeper:
    replicas: 3
    storage:
      type: persistent-claim
      size: 10Gi
      deleteClaim: false
      class: gp3

    resources:
      requests:
        memory: 1Gi
        cpu: 200m
      limits:
        memory: 2Gi
        cpu: 500m

  entityOperator:
    topicOperator: {}
    userOperator: {}
```

Deploy Kafka:

```bash
kubectl apply -f kafka-cluster.yaml --context=primary
```

### Step 3.3: Deploy Core Microservices

Create the trading engine service:

```yaml
# services/trading-engine/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: trading-engine
  namespace: default
  labels:
    app: trading-engine
    version: v1
spec:
  replicas: 3
  selector:
    matchLabels:
      app: trading-engine
      version: v1
  template:
    metadata:
      labels:
        app: trading-engine
        version: v1
      annotations:
        sidecar.istio.io/inject: "true"
    spec:
      serviceAccountName: trading-engine-sa
      containers:
        - name: trading-engine
          image: your-registry/trading-engine:v1.0.0
          ports:
            - containerPort: 8080
              name: http
            - containerPort: 8090
              name: grpc

          env:
            - name: KAFKA_BROKERS
              value: "globalconnect-kafka-kafka-bootstrap.kafka.svc.cluster.local:9092"
            - name: DATABASE_URL
              valueFrom:
                secretKeyRef:
                  name: database-credentials
                  key: url
            - name: REDIS_URL
              value: "redis-master.redis.svc.cluster.local:6379"
            - name: ENVIRONMENT
              value: "production"
            - name: REGION
              value: "us-east-1"

          resources:
            requests:
              memory: "512Mi"
              cpu: "500m"
            limits:
              memory: "1Gi"
              cpu: "1000m"

          livenessProbe:
            httpGet:
              path: /health
              port: 8080
            initialDelaySeconds: 30
            periodSeconds: 10

          readinessProbe:
            httpGet:
              path: /ready
              port: 8080
            initialDelaySeconds: 5
            periodSeconds: 5

          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
            allowPrivilegeEscalation: false
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - ALL

---
apiVersion: v1
kind: Service
metadata:
  name: trading-engine
  labels:
    app: trading-engine
spec:
  ports:
    - port: 8080
      targetPort: 8080
      name: http
    - port: 8090
      targetPort: 8090
      name: grpc
  selector:
    app: trading-engine

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: trading-engine-sa
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::ACCOUNT:role/TradingEngineRole
```

Deploy microservices:

```bash
# Deploy to primary cluster
kubectl apply -f services/trading-engine/ --context=primary

# Deploy to secondary cluster (with modifications for standby mode)
kubectl apply -f services/trading-engine/ --context=secondary

# Deploy to EU cluster (with GDPR configurations)
kubectl apply -f services/trading-engine/ --context=eu
```

### Step 3.4: Deploy Additional Core Services

```bash
# Create all service namespaces
kubectl create namespace trading --context=primary
kubectl create namespace payments --context=primary
kubectl create namespace compliance --context=primary
kubectl create namespace customer --context=primary

# Deploy all microservices (you would have similar deployments for each)
services=(
  "risk-management"
  "payment-processor"
  "fraud-detection"
  "customer-identity"
  "market-data"
  "portfolio-management"
  "notification-service"
  "audit-trail"
  "regulatory-reporting"
)

for service in "${services[@]}"; do
  kubectl apply -f services/$service/ --context=primary
  kubectl apply -f services/$service/ --context=secondary
  kubectl apply -f services/$service/ --context=eu
done
```

<div className="progress-indicator">
  <div className="progress-step completed">
    <span className="step-number">1</span>
    <span className="step-title">Multi-Region VPCs</span>
  </div>
  <div className="progress-step completed">
    <span className="step-number">2</span>
    <span className="step-title">EKS Clusters</span>
  </div>
  <div className="progress-step completed">
    <span className="step-number">3</span>
    <span className="step-title">Microservices</span>
  </div>
</div>

---

## Phase 4: Database Layer & Storage üóÑÔ∏è

### Step 4.1: Deploy PostgreSQL with Multi-Region Replication

```hcl
# Add to main.tf
resource "aws_db_subnet_group" "primary" {
  provider = aws.us_east_1
  name     = "globalconnect-primary-subnet-group"
  subnet_ids = module.primary_vpc.database_subnets

  tags = {
    Name = "GlobalConnect Primary DB subnet group"
  }
}

resource "aws_db_instance" "primary" {
  provider = aws.us_east_1

  identifier = "globalconnect-primary-db"

  # Engine configuration
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = "db.r6g.2xlarge"  # Production-grade

  # Storage
  allocated_storage     = 100
  max_allocated_storage = 1000
  storage_type         = "gp3"
  storage_encrypted    = true
  kms_key_id          = aws_kms_key.primary_db.arn

  # Database configuration
  db_name  = "globalconnect"
  username = var.db_username
  password = var.db_password
  port     = 5432

  # High Availability
  multi_az               = true
  backup_retention_period = 30
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"

  # Security
  vpc_security_group_ids = [aws_security_group.database.id]
  db_subnet_group_name   = aws_db_subnet_group.primary.name

  # Monitoring
  monitoring_interval = 60
  monitoring_role_arn = aws_iam_role.rds_monitoring.arn

  # Performance Insights
  performance_insights_enabled = true
  performance_insights_retention_period = 7

  # Deletion protection for production
  deletion_protection = true
  skip_final_snapshot = false
  final_snapshot_identifier = "globalconnect-primary-final-snapshot"

  tags = {
    Environment = "production"
    Role       = "primary-database"
  }
}

# Read replica in secondary region
resource "aws_db_instance" "secondary_replica" {
  provider = aws.us_west_2

  identifier = "globalconnect-secondary-replica"

  # Replica configuration
  replicate_source_db = aws_db_instance.primary.id

  # Instance configuration
  instance_class = "db.r6g.2xlarge"

  # Security
  vpc_security_group_ids = [aws_security_group.database_secondary.id]

  # Monitoring
  monitoring_interval = 60
  monitoring_role_arn = aws_iam_role.rds_monitoring_secondary.arn

  tags = {
    Environment = "production"
    Role       = "secondary-replica"
  }
}
```

### Step 4.2: Deploy DynamoDB Global Tables

```hcl
# DynamoDB Global Tables for real-time data
resource "aws_dynamodb_table" "trading_positions" {
  provider = aws.us_east_1

  name           = "trading-positions"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "user_id"
  range_key      = "position_id"
  stream_enabled = true
  stream_view_type = "NEW_AND_OLD_IMAGES"

  attribute {
    name = "user_id"
    type = "S"
  }

  attribute {
    name = "position_id"
    type = "S"
  }

  attribute {
    name = "symbol"
    type = "S"
  }

  global_secondary_index {
    name            = "symbol-index"
    hash_key        = "symbol"
    projection_type = "ALL"
  }

  # Enable point-in-time recovery
  point_in_time_recovery {
    enabled = true
  }

  # Server-side encryption
  server_side_encryption {
    enabled = true
    kms_key_id = aws_kms_key.dynamodb.arn
  }

  tags = {
    Environment = "production"
    Service     = "trading"
  }
}

# Replica table in secondary region
resource "aws_dynamodb_table" "trading_positions_replica" {
  provider = aws.us_west_2

  name           = "trading-positions"
  billing_mode   = "PAY_PER_REQUEST"
  hash_key       = "user_id"
  range_key      = "position_id"
  stream_enabled = true
  stream_view_type = "NEW_AND_OLD_IMAGES"

  attribute {
    name = "user_id"
    type = "S"
  }

  attribute {
    name = "position_id"
    type = "S"
  }

  attribute {
    name = "symbol"
    type = "S"
  }

  global_secondary_index {
    name            = "symbol-index"
    hash_key        = "symbol"
    projection_type = "ALL"
  }

  point_in_time_recovery {
    enabled = true
  }

  server_side_encryption {
    enabled = true
    kms_key_id = aws_kms_key.dynamodb_secondary.arn
  }

  tags = {
    Environment = "production"
    Service     = "trading"
    Region      = "secondary"
  }
}

# Global table configuration
resource "aws_dynamodb_global_table" "trading_positions" {
  depends_on = [
    aws_dynamodb_table.trading_positions,
    aws_dynamodb_table.trading_positions_replica
  ]

  name = "trading-positions"

  replica {
    region_name = "us-east-1"
  }

  replica {
    region_name = "us-west-2"
  }
}
```

### Step 4.3: Deploy Redis Cluster for Caching

```bash
# Deploy Redis using Helm
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# Create namespace
kubectl create namespace redis --context=primary

# Deploy Redis cluster
helm install redis bitnami/redis-cluster \
  --namespace redis \
  --context=primary \
  --set cluster.nodes=6 \
  --set cluster.replicas=1 \
  --set persistence.enabled=true \
  --set persistence.size=20Gi \
  --set persistence.storageClass=gp3 \
  --set auth.enabled=true \
  --set auth.password="$(openssl rand -base64 32)"

# Deploy Redis in secondary region
kubectl create namespace redis --context=secondary
helm install redis bitnami/redis-cluster \
  --namespace redis \
  --context=secondary \
  --set cluster.nodes=6 \
  --set cluster.replicas=1 \
  --set persistence.enabled=true \
  --set persistence.size=20Gi
```

### Step 4.4: Configure Database Secrets

```bash
# Create database connection secrets
kubectl create secret generic database-credentials \
  --from-literal=url="postgresql://username:password@globalconnect-primary-db.region.rds.amazonaws.com:5432/globalconnect" \
  --context=primary

kubectl create secret generic database-credentials \
  --from-literal=url="postgresql://username:password@globalconnect-secondary-replica.region.rds.amazonaws.com:5432/globalconnect" \
  --context=secondary

# Create Redis credentials
REDIS_PASSWORD=$(kubectl get secret redis -o jsonpath="{.data.redis-password}" --context=primary | base64 --decode)
kubectl create secret generic redis-credentials \
  --from-literal=password="$REDIS_PASSWORD" \
  --context=primary
```

---

## Phase 5: Monitoring & Observability üìä

### Step 5.1: Deploy Prometheus & Grafana Stack

```bash
# Add Prometheus Helm repository
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update

# Create monitoring namespace
kubectl create namespace monitoring --context=primary

# Deploy Prometheus Operator
helm install prometheus-operator prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --context=primary \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName=gp3 \
  --set prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage=50Gi \
  --set prometheus.prometheusSpec.retention=30d \
  --set grafana.adminPassword="$(openssl rand -base64 32)"
```

### Step 5.2: Configure Custom Metrics for Financial Services

```yaml
# monitoring/trading-metrics.yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: trading-engine-metrics
  namespace: monitoring
  labels:
    app: trading-engine
spec:
  selector:
    matchLabels:
      app: trading-engine
  endpoints:
    - port: metrics
      interval: 15s
      path: /metrics
---
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: trading-alerts
  namespace: monitoring
spec:
  groups:
    - name: trading.rules
      rules:
        - alert: HighTradeLatency
          expr: histogram_quantile(0.95, trading_engine_trade_duration_seconds) > 0.1
          for: 2m
          labels:
            severity: warning
            service: trading-engine
          annotations:
            summary: "High trade execution latency detected"
            description: "95th percentile trade latency is {{ $value }}s"

        - alert: TradingEngineDown
          expr: up{job="trading-engine"} == 0
          for: 1m
          labels:
            severity: critical
            service: trading-engine
          annotations:
            summary: "Trading engine is down"
            description: "Trading engine has been down for more than 1 minute"

        - alert: HighErrorRate
          expr: rate(trading_engine_errors_total[5m]) > 0.01
          for: 5m
          labels:
            severity: warning
            service: trading-engine
          annotations:
            summary: "High error rate in trading engine"
            description: "Error rate is {{ $value }} errors per second"
```

Deploy monitoring configuration:

```bash
kubectl apply -f monitoring/trading-metrics.yaml --context=primary
```

### Step 5.3: Configure Distributed Tracing with Jaeger

```bash
# Deploy Jaeger
kubectl create namespace jaeger --context=primary

# Deploy Jaeger operator
kubectl apply -f https://github.com/jaegertracing/jaeger-operator/releases/download/v1.48.0/jaeger-operator.yaml -n jaeger --context=primary

# Create Jaeger instance
cat <<EOF | kubectl apply -f - --context=primary
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: globalconnect-jaeger
  namespace: jaeger
spec:
  strategy: production
  storage:
    type: elasticsearch
    elasticsearch:
      nodeCount: 3
      storage:
        storageClassName: gp3
        size: 20Gi
  collector:
    maxReplicas: 5
    resources:
      limits:
        cpu: 200m
        memory: 256Mi
EOF
```

### Step 5.4: Set Up Log Aggregation with Fluentd

```bash
# Deploy Fluentd for log collection
helm repo add fluent https://fluent.github.io/helm-charts
helm repo update

kubectl create namespace logging --context=primary

helm install fluentd fluent/fluentd \
  --namespace logging \
  --context=primary \
  --set output.cloudWatch.enabled=true \
  --set output.cloudWatch.region=us-east-1 \
  --set output.cloudWatch.logGroupName=/aws/eks/globalconnect/logs
```

---

## Phase 6: Security & Compliance üîí

### Step 6.1: Implement Pod Security Standards

```yaml
# security/pod-security-policy.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: trading
  labels:
    pod-security.kubernetes.io/enforce: restricted
    pod-security.kubernetes.io/audit: restricted
    pod-security.kubernetes.io/warn: restricted
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all-ingress
  namespace: trading
spec:
  podSelector: {}
  policyTypes:
    - Ingress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-trading-engine
  namespace: trading
spec:
  podSelector:
    matchLabels:
      app: trading-engine
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: api-gateway
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - namespaceSelector:
            matchLabels:
              name: kafka
      ports:
        - protocol: TCP
          port: 9092
    - to: []
      ports:
        - protocol: TCP
          port: 5432 # PostgreSQL
        - protocol: TCP
          port: 6379 # Redis
```

### Step 6.2: Configure Vault for Secrets Management

```bash
# Deploy HashiCorp Vault
helm repo add hashicorp https://helm.releases.hashicorp.com
helm repo update

kubectl create namespace vault --context=primary

helm install vault hashicorp/vault \
  --namespace vault \
  --context=primary \
  --set server.ha.enabled=true \
  --set server.ha.replicas=3 \
  --set server.dataStorage.enabled=true \
  --set server.dataStorage.size=10Gi \
  --set server.dataStorage.storageClass=gp3 \
  --set ui.enabled=true
```

Initialize and configure Vault:

```bash
# Initialize Vault
kubectl exec vault-0 -n vault --context=primary -- vault operator init

# Configure Kubernetes auth
kubectl exec vault-0 -n vault --context=primary -- vault auth enable kubernetes

# Create policy for trading services
kubectl exec vault-0 -n vault --context=primary -- vault policy write trading-policy - <<EOF
path "secret/data/trading/*" {
  capabilities = ["read"]
}
path "database/creds/trading-role" {
  capabilities = ["read"]
}
EOF
```

### Step 6.3: Implement Compliance Scanning

```bash
# Deploy Falco for runtime security
helm repo add falcosecurity https://falcosecurity.github.io/charts
helm repo update

kubectl create namespace falco --context=primary

helm install falco falcosecurity/falco \
  --namespace falco \
  --context=primary \
  --set falco.grpc.enabled=true \
  --set falco.grpcOutput.enabled=true

# Deploy OPA Gatekeeper for policy enforcement
kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/release-3.14/deploy/gatekeeper.yaml --context=primary
```

Create compliance policies:

```yaml
# security/require-security-context.yaml
apiVersion: templates.gatekeeper.sh/v1beta1
kind: ConstraintTemplate
metadata:
  name: requiresecuritycontext
spec:
  crd:
    spec:
      names:
        kind: RequireSecurityContext
      validation:
        openAPIV3Schema:
          type: object
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package requiresecuritycontext

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.securityContext.runAsNonRoot
          msg := "Container must run as non-root user"
        }

        violation[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          not container.securityContext.readOnlyRootFilesystem
          msg := "Container must have read-only root filesystem"
        }
---
apiVersion: constraints.gatekeeper.sh/v1beta1
kind: RequireSecurityContext
metadata:
  name: must-have-security-context
spec:
  match:
    kinds:
      - apiGroups: ["apps"]
        kinds: ["Deployment"]
    namespaces: ["trading", "payments", "compliance"]
```

Deploy compliance policies:

```bash
kubectl apply -f security/require-security-context.yaml --context=primary
```

---

## Phase 7: Disaster Recovery & Backup üîÑ

### Step 7.1: Configure Automated Backups

```yaml
# backup/velero-backup.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: velero-backup-config
  namespace: velero
data:
  backup-schedule.yaml: |
    apiVersion: velero.io/v1
    kind: Schedule
    metadata:
      name: daily-backup
      namespace: velero
    spec:
      schedule: "0 1 * * *"  # Daily at 1 AM
      template:
        includedNamespaces:
        - trading
        - payments
        - compliance
        - kafka
        - redis
        includedResources:
        - '*'
        excludedResources:
        - events
        - events.events.k8s.io
        ttl: 720h  # 30 days
        storageLocation: default
        snapshotVolumes: true
```

### Step 7.2: Deploy Velero for Kubernetes Backup

```bash
# Install Velero CLI
wget https://github.com/vmware-tanzu/velero/releases/download/v1.12.0/velero-v1.12.0-linux-amd64.tar.gz
tar -xvf velero-v1.12.0-linux-amd64.tar.gz
sudo mv velero-v1.12.0-linux-amd64/velero /usr/local/bin/

# Create S3 bucket for backups
aws s3 mb s3://globalconnect-backup-primary --region us-east-1
aws s3 mb s3://globalconnect-backup-secondary --region us-west-2

# Install Velero
velero install \
    --provider aws \
    --plugins velero/velero-plugin-for-aws:v1.8.0 \
    --bucket globalconnect-backup-primary \
    --backup-location-config region=us-east-1 \
    --snapshot-location-config region=us-east-1 \
    --secret-file ./credentials-velero \
    --use-volume-snapshots=true \
    --context=primary
```

### Step 7.3: Implement Database Backup Strategy

```bash
# Create database backup script
cat <<'EOF' > backup-database.sh
#!/bin/bash
set -e

TIMESTAMP=$(date +%Y%m%d_%H%M%S)
DB_HOST="globalconnect-primary-db.region.rds.amazonaws.com"
DB_NAME="globalconnect"
S3_BUCKET="globalconnect-db-backups"

# Create backup
pg_dump -h $DB_HOST -U $DB_USER -d $DB_NAME | gzip > /tmp/backup_${TIMESTAMP}.sql.gz

# Upload to S3
aws s3 cp /tmp/backup_${TIMESTAMP}.sql.gz s3://${S3_BUCKET}/daily/

# Cleanup local backup
rm /tmp/backup_${TIMESTAMP}.sql.gz

# Cleanup old backups (keep 30 days)
aws s3 ls s3://${S3_BUCKET}/daily/ | \
  awk '{print $4}' | \
  head -n -30 | \
  xargs -I {} aws s3 rm s3://${S3_BUCKET}/daily/{}

echo "Database backup completed: backup_${TIMESTAMP}.sql.gz"
EOF

chmod +x backup-database.sh
```

### Step 7.4: Configure Cross-Region Replication

```yaml
# disaster-recovery/cross-region-sync.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: config-sync
  namespace: kube-system
spec:
  schedule: "*/15 * * * *" # Every 15 minutes
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: config-sync
              image: bitnami/kubectl:latest
              command:
                - /bin/bash
                - -c
                - |
                  # Sync critical configurations to secondary region
                  kubectl get configmaps -n trading -o yaml --context=primary | \
                    kubectl apply -f - --context=secondary

                  kubectl get secrets -n trading -o yaml --context=primary | \
                    kubectl apply -f - --context=secondary

                  echo "Configuration sync completed"
          restartPolicy: OnFailure
```

---

## Phase 8: Performance Optimization üöÄ

### Step 8.1: Configure Horizontal Pod Autoscaler

```yaml
# autoscaling/trading-engine-hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: trading-engine-hpa
  namespace: trading
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-engine
  minReplicas: 3
  maxReplicas: 50
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 80
    - type: Pods
      pods:
        metric:
          name: trading_requests_per_second
        target:
          type: AverageValue
          averageValue: "100"
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
        - type: Pods
          value: 4
          periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
        - type: Pods
          value: 2
          periodSeconds: 60
```

### Step 8.2: Implement Vertical Pod Autoscaler

```bash
# Install VPA
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler/
./hack/vpa-install.sh --context=primary
```

```yaml
# autoscaling/trading-engine-vpa.yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: trading-engine-vpa
  namespace: trading
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: trading-engine
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
      - containerName: trading-engine
        minAllowed:
          cpu: 100m
          memory: 128Mi
        maxAllowed:
          cpu: 2
          memory: 4Gi
        controlledResources: ["cpu", "memory"]
```

### Step 8.3: Configure Cluster Autoscaler

```yaml
# autoscaling/cluster-autoscaler.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8085"
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
        - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.28.0
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 300Mi
            requests:
              cpu: 100m
              memory: 300Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/globalconnect-primary
            - --balance-similar-node-groups
            - --skip-nodes-with-system-pods=false
          env:
            - name: AWS_REGION
              value: us-east-1
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt
              readOnly: true
          imagePullPolicy: "Always"
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
```

---

## Phase 9: Testing & Validation ‚úÖ

### Step 9.1: Deploy Load Testing Infrastructure

```bash
# Install K6 for load testing
curl https://github.com/grafana/k6/releases/download/v0.46.0/k6-v0.46.0-linux-amd64.tar.gz -L | tar xvz --strip-components 1
sudo mv k6 /usr/local/bin/
```

```javascript
// load-tests/trading-load-test.js
import http from "k6/http";
import { check, sleep } from "k6";
import { Rate } from "k6/metrics";

export let errorRate = new Rate("errors");

export let options = {
  stages: [
    { duration: "2m", target: 100 }, // Ramp up to 100 users
    { duration: "5m", target: 100 }, // Stay at 100 users
    { duration: "2m", target: 200 }, // Ramp up to 200 users
    { duration: "5m", target: 200 }, // Stay at 200 users
    { duration: "2m", target: 0 }, // Ramp down to 0 users
  ],
  thresholds: {
    http_req_duration: ["p(95)<500"], // 95% of requests must complete below 500ms
    errors: ["rate<0.01"], // Error rate must be below 1%
  },
};

const BASE_URL = "https://api.globalconnect.com";

export default function () {
  // Simulate trading operations
  let loginResponse = http.post(`${BASE_URL}/auth/login`, {
    email: "trader@example.com",
    password: "password123",
  });

  check(loginResponse, {
    "login successful": (r) => r.status === 200,
  }) || errorRate.add(1);

  if (loginResponse.status === 200) {
    let token = loginResponse.json("token");
    let headers = { Authorization: `Bearer ${token}` };

    // Get market data
    let marketData = http.get(`${BASE_URL}/market/prices`, { headers });
    check(marketData, {
      "market data retrieved": (r) => r.status === 200,
    }) || errorRate.add(1);

    // Place a trade
    let tradeResponse = http.post(
      `${BASE_URL}/trades`,
      {
        symbol: "AAPL",
        quantity: 100,
        type: "market",
        side: "buy",
      },
      { headers }
    );

    check(tradeResponse, {
      "trade placed successfully": (r) => r.status === 201,
    }) || errorRate.add(1);

    // Get portfolio
    let portfolio = http.get(`${BASE_URL}/portfolio`, { headers });
    check(portfolio, {
      "portfolio retrieved": (r) => r.status === 200,
    }) || errorRate.add(1);
  }

  sleep(1);
}
```

Run load tests:

```bash
k6 run load-tests/trading-load-test.js
```

### Step 9.2: Chaos Engineering with Chaos Monkey

```bash
# Install Chaos Monkey for Kubernetes
helm repo add chaos-mesh https://charts.chaos-mesh.org
helm repo update

kubectl create namespace chaos-engineering --context=primary

helm install chaos-mesh chaos-mesh/chaos-mesh \
  --namespace chaos-engineering \
  --context=primary \
  --set chaosDaemon.runtime=containerd \
  --set chaosDaemon.socketPath=/run/containerd/containerd.sock \
  --set dashboard.securityMode=false
```

```yaml
# chaos-tests/pod-failure-experiment.yaml
apiVersion: chaos-mesh.org/v1alpha1
kind: PodChaos
metadata:
  name: trading-engine-failure
  namespace: chaos-engineering
spec:
  action: pod-failure
  mode: one
  duration: "60s"
  selector:
    namespaces:
      - trading
    labelSelectors:
      app: trading-engine
  scheduler:
    cron: "0 */2 * * *" # Run every 2 hours
---
apiVersion: chaos-mesh.org/v1alpha1
kind: NetworkChaos
metadata:
  name: database-latency
  namespace: chaos-engineering
spec:
  action: delay
  mode: all
  selector:
    namespaces:
      - trading
    labelSelectors:
      app: trading-engine
  delay:
    latency: "100ms"
    correlation: "100"
    jitter: "0ms"
  direction: to
  target:
    mode: all
    selector:
      namespaces:
        - default
      labelSelectors:
        app: postgres
  duration: "5m"
```

### Step 9.3: Implement Health Checks

```yaml
# monitoring/health-checks.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-check-script
  namespace: monitoring
data:
  health-check.sh: |
    #!/bin/bash

    # Check trading engine health
    TRADING_HEALTH=$(curl -s -o /dev/null -w "%{http_code}" http://trading-engine:8080/health)
    if [ $TRADING_HEALTH -ne 200 ]; then
      echo "CRITICAL: Trading engine health check failed"
      exit 2
    fi

    # Check database connectivity
    if ! pg_isready -h postgres -p 5432; then
      echo "CRITICAL: Database is not ready"
      exit 2
    fi

    # Check Kafka connectivity
    if ! kafka-broker-api-versions --bootstrap-server kafka:9092 &>/dev/null; then
      echo "CRITICAL: Kafka is not accessible"
      exit 2
    fi

    # Check Redis connectivity
    if ! redis-cli -h redis ping &>/dev/null; then
      echo "CRITICAL: Redis is not accessible"
      exit 2
    fi

    echo "OK: All services are healthy"
    exit 0
```

---

## Phase 10: Go-Live Checklist & Operations üéØ

### Step 10.1: Pre-Production Checklist

```bash
# Create go-live checklist script
cat <<'EOF' > scripts/go-live-checklist.sh
#!/bin/bash
set -e

echo "üöÄ GlobalConnect Go-Live Checklist"
echo "=================================="

# Check cluster health
echo "‚úÖ Checking cluster health..."
kubectl get nodes --context=primary
kubectl get nodes --context=secondary
kubectl get nodes --context=eu

# Check all services are running
echo "‚úÖ Checking service status..."
kubectl get pods -A --context=primary | grep -v Running | wc -l
if [ $? -eq 0 ]; then
  echo "All pods are running in primary cluster"
fi

# Check database connectivity
echo "‚úÖ Checking database connectivity..."
kubectl exec -it $(kubectl get pods -l app=trading-engine -o jsonpath='{.items[0].metadata.name}' --context=primary) -- \
  psql -h $DB_HOST -U $DB_USER -d $DB_NAME -c "SELECT 1;"

# Check Kafka topics
echo "‚úÖ Checking Kafka topics..."
kubectl exec -it globalconnect-kafka-0 -n kafka --context=primary -- \
  kafka-topics --bootstrap-server localhost:9092 --list

# Check monitoring systems
echo "‚úÖ Checking monitoring..."
curl -s http://prometheus-server/api/v1/query?query=up | jq '.data.result | length'

# Check backup systems
echo "‚úÖ Checking backup status..."
velero backup get --context=primary

# Performance baseline test
echo "‚úÖ Running performance baseline..."
k6 run --quiet --summary-trend-stats="min,med,max,p(95)" load-tests/baseline-test.js

echo "üéâ Go-live checklist completed!"
EOF

chmod +x scripts/go-live-checklist.sh
```

### Step 10.2: Operational Playbooks

````markdown
# operations/incident-response-playbook.md

## Incident Response Playbook

### Severity Levels

**P0 - Critical (Trading Halt)**

- Trading engine completely down
- Database unavailable
- Security breach detected
- Response time: < 5 minutes

**P1 - High (Degraded Service)**

- Elevated error rates (>1%)
- High latency (>500ms p95)
- Single region failure
- Response time: < 15 minutes

**P2 - Medium (Performance Impact)**

- Moderate latency increase
- Non-critical service degradation
- Response time: < 1 hour

### Response Procedures

#### P0 - Trading Engine Down

1. **Immediate Actions (0-5 minutes)**

   ```bash
   # Check cluster status
   kubectl get pods -l app=trading-engine --context=primary

   # Check recent events
   kubectl get events --sort-by='.lastTimestamp' --context=primary

   # Scale up replicas immediately
   kubectl scale deployment trading-engine --replicas=10 --context=primary
   ```
````

2. **Failover to Secondary Region (5-10 minutes)**

   ```bash
   # Activate secondary region
   kubectl patch deployment trading-engine \
     -p '{"spec":{"replicas":5}}' \
     --context=secondary

   # Update load balancer
   aws elbv2 modify-target-group --target-group-arn $TG_ARN \
     --health-check-path /health
   ```

3. **Communication (10-15 minutes)**
   - Notify stakeholders via Slack
   - Update status page
   - Prepare customer communication

#### Database Failover Procedure

```bash
# Promote read replica to primary
aws rds promote-read-replica \
  --db-instance-identifier globalconnect-secondary-replica

# Update application configuration
kubectl patch configmap database-config \
  -p '{"data":{"primary_host":"new-primary-endpoint"}}' \
  --context=primary

# Restart affected services
kubectl rollout restart deployment/trading-engine --context=primary
```

### Monitoring & Alerting

#### Key Metrics Dashboard

- Trading engine latency (p95, p99)
- Trade throughput (trades/second)
- Error rates by service
- Database connection pool utilization
- Kafka consumer lag
- Active user sessions
- Revenue per minute

#### Alert Thresholds

```yaml
# Critical Alerts
trading_engine_down: 0 replicas available
database_connections_exhausted: >95% pool utilization
kafka_consumer_lag: >10,000 messages behind
trade_error_rate: >0.5% over 5 minutes

# Warning Alerts
high_latency: p95 >200ms over 5 minutes
memory_pressure: >85% memory utilization
disk_space_low: <20% available storage
certificate_expiry: <30 days until expiration
```

### Runbook Templates

#### High Memory Usage

```bash
# 1. Identify memory consumers
kubectl top pods --sort-by=memory --context=primary

# 2. Check for memory leaks
kubectl logs -f deployment/trading-engine --context=primary | grep -i "memory\|oom"

# 3. Scale horizontally if needed
kubectl scale deployment trading-engine --replicas=8 --context=primary

# 4. Restart pods showing memory issues
kubectl rollout restart deployment/trading-engine --context=primary
```

#### Database Performance Issues

```bash
# 1. Check active connections
psql -h $DB_HOST -c "SELECT count(*) FROM pg_stat_activity;"

# 2. Identify slow queries
psql -h $DB_HOST -c "SELECT query, query_start, state FROM pg_stat_activity WHERE state = 'active' ORDER BY query_start;"

# 3. Check for locks
psql -h $DB_HOST -c "SELECT blocked_locks.pid AS blocked_pid, blocked_activity.usename AS blocked_user, blocking_locks.pid AS blocking_pid, blocking_activity.usename AS blocking_user FROM pg_catalog.pg_locks blocked_locks JOIN pg_catalog.pg_stat_activity blocked_activity ON blocked_activity.pid = blocked_locks.pid JOIN pg_catalog.pg_locks blocking_locks ON blocking_locks.locktype = blocked_locks.locktype AND blocking_locks.DATABASE IS NOT DISTINCT FROM blocked_locks.DATABASE AND blocking_locks.relation IS NOT DISTINCT FROM blocked_locks.relation WHERE NOT blocked_locks.GRANTED;"

# 4. Restart read replicas if needed
aws rds reboot-db-instance --db-instance-identifier globalconnect-secondary-replica
```

````

### Step 10.3: Operational Dashboards

Create comprehensive monitoring dashboards:

```yaml
# monitoring/grafana-dashboard-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: trading-dashboard
  namespace: monitoring
data:
  dashboard.json: |
    {
      "dashboard": {
        "title": "GlobalConnect Trading Platform",
        "panels": [
          {
            "title": "Trade Execution Latency",
            "type": "graph",
            "targets": [
              {
                "expr": "histogram_quantile(0.95, trading_engine_trade_duration_seconds_bucket)",
                "legendFormat": "95th Percentile"
              },
              {
                "expr": "histogram_quantile(0.99, trading_engine_trade_duration_seconds_bucket)",
                "legendFormat": "99th Percentile"
              }
            ],
            "yAxes": [
              {
                "label": "Seconds",
                "max": 1
              }
            ],
            "alert": {
              "conditions": [
                {
                  "query": {
                    "queryType": "",
                    "refId": "A"
                  },
                  "reducer": {
                    "type": "last",
                    "params": []
                  },
                  "evaluator": {
                    "params": [0.1],
                    "type": "gt"
                  }
                }
              ],
              "executionErrorState": "alerting",
              "noDataState": "no_data",
              "frequency": "30s",
              "handler": 1,
              "name": "High Trade Latency",
              "message": "Trade execution latency is above acceptable threshold"
            }
          },
          {
            "title": "Trading Volume",
            "type": "stat",
            "targets": [
              {
                "expr": "sum(rate(trading_engine_trades_total[5m]))",
                "legendFormat": "Trades/sec"
              }
            ]
          },
          {
            "title": "Active Users",
            "type": "stat",
            "targets": [
              {
                "expr": "trading_engine_active_sessions",
                "legendFormat": "Active Sessions"
              }
            ]
          },
          {
            "title": "Error Rate by Service",
            "type": "table",
            "targets": [
              {
                "expr": "sum by (service) (rate(http_requests_total{status=~\"5..\"}[5m])) / sum by (service) (rate(http_requests_total[5m]))",
                "format": "table"
              }
            ]
          }
        ]
      }
    }
````

### Step 10.4: Documentation & Knowledge Transfer

Create comprehensive documentation:

```markdown
# docs/architecture-overview.md

## GlobalConnect Architecture Overview

### System Components

#### Core Services

- **Trading Engine**: High-frequency trading execution
- **Risk Management**: Real-time position and exposure monitoring
- **Market Data Service**: Live market feed processing
- **Portfolio Management**: Position tracking and reporting
- **Payment Processor**: Settlement and money movement
- **Customer Identity**: Authentication and KYC
- **Fraud Detection**: ML-based anomaly detection
- **Audit Trail**: Immutable transaction logging
- **Regulatory Reporting**: Compliance data aggregation

#### Infrastructure Components

- **EKS Clusters**: Kubernetes orchestration across 3 regions
- **RDS PostgreSQL**: Primary transactional database
- **DynamoDB**: High-speed position data
- **ElastiCache Redis**: Session and market data caching
- **Apache Kafka**: Event streaming backbone
- **Istio Service Mesh**: Traffic management and security
- **Prometheus/Grafana**: Monitoring and alerting
- **Vault**: Secrets management
- **Velero**: Backup and disaster recovery

### Data Flow Architecture
```

Client Request ‚Üí API Gateway ‚Üí Load Balancer ‚Üí Trading Engine ‚Üí Risk Engine
‚Üì
Market Data Feed ‚Üí Kafka ‚Üí Multiple Consumers ‚Üí Database Updates
‚Üì  
Trade Execution ‚Üí Audit Trail ‚Üí Regulatory Reporting ‚Üí External APIs

```

### Security Model
- Zero-trust network architecture
- mTLS between all services
- Pod security policies enforced
- Secrets encrypted at rest and in transit
- Regular security scanning and compliance audits
- RBAC with principle of least privilege

### Scalability Patterns
- Horizontal pod autoscaling based on CPU, memory, and custom metrics
- Cluster autoscaling for node capacity
- Database read replicas for query distribution
- CDN for static content delivery
- Kafka partitioning for parallel processing

### Disaster Recovery
- RTO: 15 minutes
- RPO: 5 minutes
- Multi-region active-passive setup
- Automated failover procedures
- Regular DR testing schedule
```

## Step 10.5: Cost Optimization

Implement cost monitoring and optimization:

```bash
# Install AWS Cost Explorer CLI
pip install aws-cost-explorer

# Create cost monitoring script
cat <<'EOF' > scripts/cost-monitoring.sh
#!/bin/bash

# Get current month costs by service
aws ce get-cost-and-usage \
  --time-period Start=2025-07-01,End=2025-07-31 \
  --granularity MONTHLY \
  --metrics BlendedCost \
  --group-by Type=DIMENSION,Key=SERVICE

# Get EKS-specific costs
aws ce get-cost-and-usage \
  --time-period Start=2025-07-01,End=2025-07-31 \
  --granularity DAILY \
  --metrics BlendedCost \
  --filter file://eks-cost-filter.json

# Alert if costs exceed threshold
CURRENT_COST=$(aws ce get-cost-and-usage --time-period Start=2025-07-01,End=2025-07-31 --granularity MONTHLY --metrics BlendedCost | jq -r '.ResultsByTime[0].Total.BlendedCost.Amount')

if (( $(echo "$CURRENT_COST > 5000" | bc -l) )); then
  echo "WARNING: Monthly costs ($CURRENT_COST) exceed threshold"
  # Send alert to Slack/email
fi
EOF
```

Cost optimization strategies:

```yaml
# cost-optimization/scheduled-scaling.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-down-non-trading-hours
spec:
  schedule: "0 18 * * 1-5" # 6 PM weekdays
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: scaler
              image: bitnami/kubectl:latest
              command:
                - /bin/bash
                - -c
                - |
                  # Scale down non-critical services after market close
                  kubectl scale deployment risk-management --replicas=2
                  kubectl scale deployment portfolio-management --replicas=1
                  kubectl scale deployment notification-service --replicas=1

                  # Scale down node groups
                  aws autoscaling update-auto-scaling-group \
                    --auto-scaling-group-name eks-general-nodes \
                    --min-size 1 --desired-capacity 2
          restartPolicy: OnFailure
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: scale-up-trading-hours
spec:
  schedule: "30 8 * * 1-5" # 8:30 AM weekdays
  jobTemplate:
    spec:
      template:
        spec:
          containers:
            - name: scaler
              image: bitnami/kubectl:latest
              command:
                - /bin/bash
                - -c
                - |
                  # Scale up for market open
                  kubectl scale deployment risk-management --replicas=5
                  kubectl scale deployment portfolio-management --replicas=3
                  kubectl scale deployment notification-service --replicas=3

                  # Scale up node groups
                  aws autoscaling update-auto-scaling-group \
                    --auto-scaling-group-name eks-general-nodes \
                    --min-size 2 --desired-capacity 6
          restartPolicy: OnFailure
```

## Conclusion & Next Steps üéâ

Congratulations! You've successfully built and deployed a production-ready, multi-region financial services platform on AWS. Your GlobalConnect system now includes:

### ‚úÖ What You've Accomplished

**Infrastructure & Platform**

- Multi-region AWS infrastructure spanning 3 regions
- Production-grade EKS clusters with auto-scaling
- Highly available databases with cross-region replication
- Enterprise-grade security and compliance controls

**Microservices Architecture**

- 40+ microservices for complete trading operations
- Event-driven architecture with Apache Kafka
- Service mesh with Istio for traffic management
- Comprehensive monitoring and observability

**Operations & Reliability**

- Automated backup and disaster recovery
- 99.99% availability SLA capability
- 15-minute RTO and 5-minute RPO
- Comprehensive incident response procedures

**Security & Compliance**

- Zero-trust security model
- Encrypted data at rest and in transit
- Automated compliance scanning
- Audit trails for regulatory requirements

### üöÄ Recommended Next Steps

1. **Advanced Features**

   - Implement machine learning for fraud detection
   - Add algorithmic trading capabilities
   - Integrate with external market data providers
   - Build mobile trading applications

2. **Enhanced Monitoring**

   - Custom business metrics dashboards
   - Predictive alerting with ML
   - User experience monitoring
   - Cost optimization automation

3. **Compliance & Regulations**

   - SOC 2 Type II certification
   - PCI DSS compliance implementation
   - Regional regulatory adaptations
   - Enhanced audit logging

4. **Performance Optimization**
   - Implement caching strategies
   - Database query optimization
   - CDN integration for global users
   - Advanced auto-scaling policies

### üìö Additional Resources

- [AWS Well-Architected Framework](https://aws.amazon.com/architecture/well-architected/)
- [Kubernetes Production Best Practices](https://kubernetes.io/docs/setup/best-practices/)
- [Financial Services on AWS](https://aws.amazon.com/financial-services/)
- [CNCF Cloud Native Security](https://www.cncf.io/reports/cloud-native-security-whitepaper/)

### üõ†Ô∏è Maintenance Tasks

**Daily**

- Monitor system health and performance
- Review security alerts and logs
- Check backup completion status
- Validate auto-scaling behavior

**Weekly**

- Review cost optimization opportunities
- Update security patches
- Conduct disaster recovery tests
- Performance baseline comparisons

**Monthly**

- Security compliance audits
- Capacity planning reviews
- Infrastructure cost analysis
- Documentation updates

### üí° Pro Tips

1. **Start Small**: Begin with a subset of services and gradually expand
2. **Automate Everything**: Infrastructure, deployments, testing, and operations
3. **Monitor Proactively**: Set up comprehensive alerting before issues occur
4. **Test Regularly**: Chaos engineering and disaster recovery testing
5. **Document Thoroughly**: Keep runbooks and architecture docs current
6. **Security First**: Implement security controls from day one
7. **Cost Awareness**: Monitor and optimize costs continuously

Your GlobalConnect platform is now ready to handle millions of trades with enterprise-grade reliability, security, and scalability. The foundation you've built provides a solid base for future growth and feature development.

Happy trading! üìà
